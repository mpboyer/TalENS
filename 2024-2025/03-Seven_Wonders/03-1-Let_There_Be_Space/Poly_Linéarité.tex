\documentclass{classe}
\title{Les Sept Merveilles de la Linéarité\\ \small Cours TalENS n°3-4}
\author{Clément Allard --- Matthieu Boyer}
\date{11 janvier 2025}

\usepackage[pdftex,outline]{contour}

\newcommand{\point}[3]{\draw (#1 -.1, #2 -.1) -- (#1 + .1, #2 + .1);
\draw (#1 +.1, #2 -.1) -- (#1 - .1, #2 + .1);
\draw (#1, #2) node[below right]{#3};}

\renewcommand*{\K}{\mathbb{K}}
\graphicspath{{./Images/}}
\tikzset{domaine/.style 2 args={domain=#1:#2}}

\begin{document}

\section{Découvrons la linéarité}

\subsection{Ce qui marche déjà bien}

Commençons avec un exemple simple, la géométrie euclidienne dans le plan, et essayons de le formaliser de manière un peu abstraite. On va se donner une origine $O$ et on va repérer chaque point par un vecteur : à un point $M$ on associera le vecteur $\overrightarrow{OM}$. Que peut-on faire dans le plan ? Voici quelques réponses:

\begin{itemize}
	\item On a le droit d'ajouter deux vecteurs $\overrightarrow{u}$ et $\overrightarrow{v}$, on obtient un troisième vecteur qui est toujours dans le plan, et on a $\overrightarrow{u} + \overrightarrow{v} = \overrightarrow{v} + \overrightarrow{u}$, ce qui revient à les mettre bout à bout;
	\item Il existe un vecteur qui traduit le non déplacement $\overrightarrow{0}$ appelé \emph{vecteur nul};
	\item À chaque vecteur $\overrightarrow{u}$, on peut associer un autre vecteur $\overrightarrow{-u}$ tel que $\overrightarrow{u} + \overrightarrow{-u} = \overrightarrow{0}$ : on parle d'opposé, et cela revient à aller dans la direction opposé au vecteur de départ;
	\item On a le droit de multiplier un vecteur par un nombre (on parle de multiplication par un scalaire), ce qui revient à l'agrandir;
	\item On a $(\lambda + \mu)\overrightarrow{u} = \lambda\overrightarrow{u} + \mu\overrightarrow{u}$ (distributivité scalaire) et $\lambda(\overrightarrow{u}+\overrightarrow{v}) = \lambda\overrightarrow{u}+\lambda\overrightarrow{v}$ (distributivité vectorielle).
\end{itemize}

On peut décomposer les vecteurs sur des vecteurs dits de base, et on peut leur appliquer différentes opérations, comme une projection orthogonale ou bien une rotation selon un axe. Ce formalisme fonctionne très bien pour la géométrie dans le plan, et on peut l'étendre à plein d'autres concepts mathématiques.
L'idée de ce cours est donc d'étudier un peu le concept d'espace vectoriel décrit ci-dessus, et de voir quelques une de ses plus belles applications.

\subsection{Explorons l'inexploré !}

Pour formaliser vraiment notre concept, il nous faut un petit outil abstrait, qui formalise la notion de nombre: la notion de \emph{corps}.

Un corps est un ensemble qui nous permet de faire des additions, soustractions, des multiplications et des divisions (sauf par $0$, où $0$ est défini comme $x+0 = 0+x = x$ pour $x$ un élément du corps).

\begin{définition}{Notion de Corps}{}
	Un ensemble $\K$ muni de deux lois de produit interne $+, \times$ (c'est à dire qui prennent deux éléments du corps et renvoient un élément du corps), est un corps si, et seulement si:
	\begin{itemize}
		\item L'addition est commutative: $\forall \lambda, \mu \in \K, \lambda + \mu = \mu + \lambda$
		\item Il existe un nombre $0_{\K}$ tel que $\forall \lambda, \lambda + 0_{\K}$, et $\forall \lambda, \exists -\lambda, \lambda + -\lambda = 0_{\K}$. On dit que $\left(\K, +\right)$ est un groupe et qu'il est de plus abélien.
		\item Il existe un nombre $1_{\K}$ tel que $\forall \lambda, \lambda \times 1_{\K} = \lambda = 1_{\K} \times \lambda$ et de plus, $\forall \lambda \in \K\setminus 0_{\K}, \exists \lambda^{-1}, \lambda^{-1} \times \lambda = \lambda \times \lambda^{-1} = 1_{\K}$. On dit que $\left(\K, \times\right)$ est un groupe.
		\item On a toujours: $\lambda \times \left( a + b \right) = \lambda \times a + \lambda \times b$ et $\left( a + b\right)\times \lambda = a\times \lambda + b\times \lambda$.
	\end{itemize}
\end{définition}

\begin{remarque}{Corps Commutatif}{}
	Dans la tradition mathématique française, on ne demande pas qu'un corps $\left( \K, +, \times \right)$ soit commutatif, c'est-à-dire qu'on n'a pas nécessairement une multiplication commutative.
	Un exemple de corps non commutatif est celui noté $\H$ des quaternions. On ne détaillera pas sa construction ici.
\end{remarque}

\begin{théorème}{$\R$ existe}{}
	Il existe un corps des réels noté $\R$ muni d'une addition $+$ et d'une multiplication $\times$.
\end{théorème}
\begin{proof}
	Si vous ne connaissez pas les opérations qui définisse $\R$ il est temps de s'inquiéter. Cependant, si vous cherchez à vérifier que $\R$ existe, il vaut mieux chercher des informations sur la construction de l'ensemble $\R$ comme complété de l'ensemble des suites sur $\Q$.
\end{proof}

Nous pouvons donc enfin formaliser la notion de $\R$-espace vectoriel dont nous avons parlé en introduction.

\begin{définition}{$\R$-Espace Vectoriel}{}
	Soit $E$ un ensemble. Il existe une addition $+$ et un produit extérieur $\cdot$ tels que
	\begin{itemize}
		\item On peut faire des additions sur $E$, avec existence d'un élément neutre $0_E$ et d'un opposé (que l'on note $-u$) : on a $u+v = v+u$, $u+(v+w) = (u+v)+w$, $u+0=u$ et $u + (-u) = 0$ pour $u$, $v$ et $w$ trois éléments de $E$.
		\item On peut multiplier des vecteurs par des scalaires avec le produit extérieur : $\lambda\cdot x$ est encore un élément de $E$ pour $\lambda \in \R$.
		\item La multiplication se comporte bien avec l'addition : on peut distribuer des scalaires et des vecteurs, multiplier par $1$ un vecteur ne le change pas : on a $(\lambda + \mu)\cdot u = \lambda\cdot u + \mu\cdot u$ (distributivité scalaire) et $\lambda\cdot (u+v) = \lambda \cdot u+\lambda\cdot v$ (distributivité vectorielle), $1\cdot u = u$, $(\lambda\times\mu)\cdot u = \lambda\cdot (\mu\cdot u)$ pour $u$, et $v$ éléments de $E$ et $\lambda$ et $\mu$ des réels.
	\end{itemize}
	Alors $E$ est un $\R$-espace vectoriel.
\end{définition}

Il est bien évidemment possible de remplacer $\R$ par un corps quelconque $\K$ pour obtenir une notion de $\K$-espace vectoriel.
Vous verrez par exemple en mathématiques expertes l'ensemble $\C$ des nombres complexes, qu'on peut également munir d'une structure de corps, ce qui permet de définir des $\C$-espaces vectoriels.
Quand le corps est évident (dans la suite on ne travaillera que sur des $\R$-espaces, par exemple), ou alors qu'il n'est pas vraiment utilisé, on parle juste d'espace vectoriel.

\begin{définition}{Vecteurs et scalaires}{}
	Les éléments de $E$ sont appelés vecteurs et ceux de $\R$ des scalaires.
\end{définition}

Finalement, si on veut résumer la notion de $\R$-espace vectoriel, on en arrive simplement à:
Soit $E$ un ensemble. Il existe une addition $+$ et un produit extérieur $\cdot$ telsque
\begin{itemize}
		\item On peut faire des additions sur $E$, avec existence d'un élément neutre $0_E$ tel que $x + 0_E = 0_E + x = x$ pour $x$ dans $E$, et d'un opposé.
		\item On peut multiplier des vecteurs par des scalaires avec le produit extérieur : $\lambda\cdot x$ est encore un élément de $E$ pour $\lambda \in \R$.
		\item La multiplication se comporte bien avec l'addition : on peut distribuer des scalaires et des vecteurs, multiplier par $1$ un vecteur ne le change pas.
\end{itemize}
Alors $E$ est un $\R$-espace vectoriel.

\subsection{Revenons sur $\R^2$}

Essayons de montrer que $\R^2$ ("l'espace vectoriel de la géométrie euclidienne dans le plan") est un $\R$-espace vectoriel. On note les éléments de $\R^2$ sous la forme suivante : $(x, y)$ où $x\in\R$ et $y\in\R$.

%\begin{enumerate}
%	\item En revenant à l'intuition géométrique du début (les points sont assimilés à des vecteurs par rapport à l'origine), quelles seraient les opérations d'addition et de produit extérieur que l'on pourrait prendre ?
%	\item Vérifions que ces opérations sont compatibles pour montrer que $\R^2$ avec ces opérations est un espace vectoriel.
%\end{enumerate}

Si on prend pour l'opération d'addition sur $\R^{2}$ la fonction $+: \left( x_{1}, y_{1} \right), \left( x_{2}, y_{2} \right) \in \left(\R^{2}\right)^{2} \mapsto \left( x_{1} + x_{2}, y_{1} + y_{2} \right)$, et pour le produit extérieur $\cdot: \lambda, (x, y) \in \R\times \R^{2} \mapsto \left( \lambda x, \lambda y \right)$, on munit bien $\R^{2}$ d'une structure d'espace vectoriel.
On vérifie aisément les propriétés demandées, mais surtout, on vérifie que c'est bien la même que celle décrite en introduction, en prenant $O = \left( 0, 0 \right)$.

\begin{remarque}{Affinité de $\R^{2}$}{}
	Le type d'espace obtenu en reprenant la construction de l'introduction mais sans spécifier de point particulier "origine" est appelé \emph{espace affine}.
	La construction définie ci-dessus est alors le vectorialisé en $\left(0, 0\right)$ d'un espace affine sur l'ensemble $\R^{2}$.
	Cette notion est toutefois bien plus compliquée à formaliser et demande plus de pratique de la notion de groupe.
\end{remarque}

\subsection{Autres exemples d'espaces vectoriels}

\subsubsection{$\R^n$}

On peut définir de même ce qu'on appelle "espace 3D" en géométrie euclidienne: $\R^3$ dont les vecteurs s'écrivent sous la forme $(x, y, z)$, et aller plus loin avec par exemple l'espace-temps $\R^4$ de vecteurs $(x, y, z, t)$ etc. On garde les mêmes lois que pour $\R^2$ : on a $(x, y, z) + (x', y', z') = (x+x', y+y', z+z')$ et $\lambda(x, y, z) = (\lambda x, \lambda y, \lambda z)$.

\begin{définition}{Espace Euclidiens Réels de Dimension Finie}{}
	Plus généralement, si $n \in \N^{*}$, on définit une structure d'espace vectoriel sur $\R^{n}$ de la même manière que précédemment:
	\begin{itemize}
		\item $\left( x_{1}, \ldots, x_{n} \right) + \left( y_{1}, \ldots, y_{n} \right) = \left( x_{1} + y_{1}, \ldots, x_{n} + y_{n} \right)$.
		\item $\lambda \cdot \left( x_{1}, \ldots, x_{n} \right) = \left( \lambda \cdot x_{1}, \ldots, \lambda \cdot x_{n} \right)$.
	\end{itemize}
\end{définition}

\subsubsection{Les fonctions réelles}

\begin{définition}{Fonction réelle à valeurs réelles}{}
On appelle fonction réelle à valeurs réelles toute fonction qui a un réel associe un autre réel.
\end{définition}

\begin{théorème}{Espace vectoriel des fonctions réelles à valeurs réelles}{}
	On définit $f+g$ pour $f$ et $g$ des fonctions à valeurs réelles par la relation $(f+g)(x) = f(x)+g(x)$ et $\lambda f$ par $(\lambda f)(x) = \lambda f(x)$, autrement dit l'addition et la multiplication point à point.
	On obtient que l'ensemble des fonctions réelles muni des deux opérations est un $\R$-espace vectoriel.
\end{théorème}

\subsection{Sous-espace vectoriel}

On va s'intéresser aux parties de $E$ qui profitent des propriétés géométriques des espaces vectoriels. Intuitivement, ce sont les parties de $E$ qu'on peut considérer comme des espaces vectoriels.

\begin{example}{}
	Par exemple, une droite passant par l'origine est un sous-espace vectoriel de $\R^2$.
\end{example}

\begin{définition}{Sous-Espace Vectoriel}{}
	$F \subseteq E$ est un sous-espace vectoriel de $E$ si et seulement si $0_E$ appartient à $F$ et que $F$ est stable par toute combinaison linéaire : pour tous $\lambda$, $\mu \in \K$ et $x$, $y\in F$, $\lambda x + \mu y \in F$.
\end{définition}

\begin{propositionfr}{(Sous-)Espace Vectoriel}{}
	Un sous-espace vectoriel $F$ de $\left( E, +, \cdot \right)$, est un espace vectoriel pour $+_{\mid F}, \cdot_{\mid F}$.
\end{propositionfr}

Un sous-espace vectoriel, c'est donc un espace vectoriel dans un espace vectoriel.

\section{Liberté, bases et dimension}

\begin{définition}{Famille}{}
	Une famille d'éléments indicée par un ensemble $I$ est un ensemble ordonné d'éléments $\left( x_{i} \right)_{i \in I}$.
	On dira que la famille est finie si et seulement si $I$ est fini.
\end{définition}

Ceci nous permet de définir la portée/l'espace engendré d'une famille de vecteurs. C'est l'espace dans lequel on voudra faire de la géométrie, en sachant qu'on pourra profiter de la famille.

\begin{définition}{Espace Vectoriel Engendré}{}
	L'espace vectoriel engendré $\Vect_{\K}\left( x_{i} \right)$ par une famille $\left( x_{i}\right)_{i\in I}$ est le plus petit espace vectoriel qui contient tous les $x_{i}$.
\end{définition}

\begin{propositionfr}{Caractérisation des Espaces Engendrés}{}
	Soit $\mathcal{V}\left( x_{i} \right)_{i\in I}$ l'ensemble des sous-espaces vectoriels de $E$ qui contiennent la famille $\left(x_{i}\right)_{i\in I}$.
	Alors, on a:
	\begin{equation*}
		\Vect_{\K}\left( x_{i} \right) = \bigcap_{F \in \mathcal{V}\left( x_{i} \right)} F = \left\{ \sum_{i\in I}\lambda_{i}x_{i}\ \middle| \ \left(\lambda_{i}\right)_{i\in I} \in \K^{I}\right\}
	\end{equation*}
	Autrement dit, $\Vect_{\K}\left( x_{i} \right)$
\end{propositionfr}
\begin{proof}
	La première égalité se déduit du fait que si $F_{1}, F_{2}$ sont deux sous-espaces vectoriels de $E$, alors, $F_{1} \cap F_{2}$ est un espace vectoriel.
	La seconde se déduit du fait que l'ensemble de droite est un sous-espace vectoriel de $E$ qui contient les $\left( x_{i} \right)$
\end{proof}

\begin{définition}{Famille libre, génératrice, base}{}
\begin{itemize}
\item Une famille est dite libre si, pour toute famille de scalaires $(\lambda_i)$ indexée par $I$, la condition $\sum_{i\in I} \lambda_i x_i = 0$ donne que tous les $\lambda_i$ sont nuls;
\item Une famille est dite génératrice si, pour tout élément de $E$, on peut associer une écriture sous la forme $\sum_{i\in I} \lambda_i x_i$, c'est-à-dire si $\Vect\left( x_{i} \right) = E$;
\item Une famille est une base si elle est libre et génératrice. Ceci est équivalent au fait qu'il existe, pour chaque élément de $E$ une unique écriture sous la forme $\sum_{i\in I} \lambda_i x_i$.
\end{itemize}
\end{définition}

\begin{example}
Dans $\mathbb{R}^2$, $((1, 0),(0, 1))$ forme une base de $\mathbb{R}^2$ : en effet, tout vecteur $(a, b)$ peut s'écrire sous la forme $a(1, 0) + b(0, 1)$ et si $(0, 0) = a(1, 0) + b(0, 1) = (a, b)$, alors on a nécessairement $a=b=0$
\end{example}

En substance, à chaque vecteur de la famille, on associe un "axe" qui est l'ensemble des déplacements que l'on peut faire selon cet "axe" (en allant dans la direction du vecteur).
\begin{itemize}
	\item La liberté s'illustre comme le fait que chacun de nos axes est utile : un déplacement selon cet axe ne pourrait être substitué en se déplaceant selon d'autre axes;
	\item Le caractère générateur s'illustre comme le fait qu'on puisse atteindre un point en se déplaceant selon nos axes;
	\item Si notre famille est une base, alors on a, pour tout point, une seule combinaison de vecteurs permettant d'atteindre ce point.
\end{itemize}

\begin{définition}{Dimension}{}
	En se donnant une base d'un espace vectoriel, on appelle dimension la taille de la base : ceci correspond au nombre d'indices. La dimension ne dépend pas de la base choisie.
\end{définition}
\begin{proof}
	On vérifie d'abord que si $w_{1}, \ldots, w_{n}$ engendre $V$, et que si $v_{1}, \ldots, v_{m}$ est une famille libre de $V$, alors $v_{1}, \ldots, v_{m}, w_{m + 1}, \ldots, w_{n}$ engendre $V$.
	En effet, puisque $v_{1} \in V$, on peut écrire:
	\begin{equation*}
		v_{1} = \sum_{k = 1}^{n} \lambda_{k}w_{k} \text{ et donc } w_{1} = \lambda_{1}^{-1}v_{1} - \sum_{k = 2}^{n} \lambda_{1}^{-1}\lambda_{k}w_{k}
	\end{equation*}
	Donc $v_{1}, w_{2}, \ldots, w_{n}$ engendre encore $V$.
	On recommence l'opération pour chaque $v_{i}$:
	\begin{equation*}
		v_{i} = \sum_{k = 1}^{i - 1}\mu_{k}v_{k} + \sum_{k = i}^{n}\mu_{k}w_{k}
	\end{equation*}
	et donc:
	\begin{equation*}
		w_{i} = \mu_{i}^{-1}v_{i} - \sum_{k = 1}^{i - 1}\mu_{i}^{-1}\mu_{k}v_{k} - \sum_{k = i + 1}^{n}\mu_{i}^{-1}\mu_{k}w_{k}
	\end{equation*}
	Soit $L$ une famille libre et $G$ une famille génératrice de $E$, on va montrer que $\abs{L} \leq \abs{G}$.
	On va supposer d'abord que $G$ est finie de taille $n$. Le résultat précédent nous affirme que pour toute partie finie de $L$ de cardinal $m$, on a $m \leq n$.
	Par conséquent, $L$ est finie et de cardinal inférieur ou égal à $n$.
	Le fait que si $x_{k} \notin \Vect\left( x_{1}, \ldots, x_{k - 1} \right)$ ou $x_{1}, \ldots, x_{k - 1}$ est une famille libre, alors $\Vect\left( x_{1}, \ldots, x_{k} \right)$ a pour base $x_{1}, \ldots, x_{k}$ permet de terminer la preuve.
\end{proof}

Le premier résultat démontré dans la preuve est appelé Lemme de Steinitz, et le dernier résultat est appelé théorème de la base incomplète.

Le résultat dans le cas infini demande l'utilisation de l'axiome du choix (ou du lemme des ultrafiltres), et dépasse donc très fortement le cadre de ce cours.

%TODO : preuve

\section{Applications linéaires et matrices}

Maintenant que nous avons introduit les notions de vecteurs et de bases, essayons de voir comment modéliser des transformations : une dilatation, projection orthogonale ou bien rotation selon un axe. Le formalisme pertinent est celui d'application linéaire.

\begin{définition}{Application linéaire}{}
On considère deux espaces vectoriels $E$ et $F$. On appelle application linéaire une fonction $f$ qui à un élément de $E$ associe un élément de $F$, et qui vérifie les propriétés suivantes :
\begin{itemize}
	\item Pour tous vecteurs $x$ et $y$ de $E$, $f(x+y) = f(x)+f(y)$
	\item Pour tout vecteur $x$ de E et tout scalaire $\lambda$, $f(\lambda x) = \lambda f(x)$
\end{itemize}
\end{définition}

\begin{propositionfr}{Opérations avec des applications linéaires}{}
On a
$$f(\lambda_1 x_1 + \cdots + \lambda_n x_n) = \lambda_1 f(x_1) + \cdots + \lambda_n f(x_n)$$
où $\lambda_1 \cdots \lambda_n$ sont des scalaires et $x_1 \cdots x_n$ des vecteurs.
\end{propositionfr}

\begin{théorème}{Identification d'applications linéaires}{}
Soient $E$ et $F$ deux espaces vectoriels, $(e_i)_{i\in I}$ une base de $E$ et $(y_i)_{i\in I}$ une famille de vecteurs de $F$. Il existe une unique application linéaire telle que, pour tout indice $i$, $f(e_i) = y_i$.
\end{théorème}

\begin{remarque}{Utilité}{}
Ce théorème est essentiel car il permet de comprendre les applications linéaires en s'intéressant uniquement à l'image d'une base, car par linéarité on peut trouver la valeur de \textbf{n'importe quel} vecteur.
\end{remarque}

Donnons nous un exemple pour visualiser ceci : on considère $\mathbb{R}^3$ comme espace vectoriel de départ et d'arrivée : on prend $E = F = \mathbb{R}^3$. Comment peut-on décrire la rotation d'un d'angle $\pi$ autour de l'axe $z$ ? On utilise notre théorème précédent : on sait que $f(x) = -x$, $f(y) = -y$ ainsi que $f(z) = z$. Ensuite, on utilise notre théorème précédent : pour tout vecteur $u$, on a l'existence de scalaires tels que $u = \alpha x + \beta y + \gamma z$, et donc $f(u) = \alpha f(x) + \beta f(y) + \gamma f(z)$ ce qui donne donc $f(u) = -\alpha x - \beta y + \gamma z$

%TODO : figure sympa pour illustrer la rotation

\section{Exemple des équations différentielles linéaires}

\begin{définition}{Équation différentielle linéaire d'ordre 1}{}
On appelle une équation linéaire d'ordre 1 toute équation de la forme
$$y' + a(x)y = b(x)$$
où l'inconnue $y$ est une fonction dérivable sur $I$ ($I$ un intervalle), $a$ et $b$ sont des fonctions continues sur $I$.
\end{définition}

Ce type d'équations est très présent que ce soit en physique ou bien dans d'autres domaines des sciences. Mais on peut se demander : comment les résoudre ? On va se placer à présent dans le cas où $b=0$. On cherche donc à résoudre
$$y' + a(x)y = 0$$.

On peut remarquer quelque chose : l'ensemble des solutions $S$ est un sous espace vectoriel de l'ensemble des fonctions réelles à valeurs réelles !

Mais donc comment trouver une solution ? On considère une fonction $f$ sous la forme $f(x) = g(x)\mathrm{e}^{h(x)}$. On a $f$ dérivable avec $f'(x) = g'(x)\mathrm{e}^{h(x)} + g(x)h'(x)\mathrm{e}^{h(x)} = (g'(x) + g(x)h'(x))\mathrm{e}^{h(x)}$.

Ici on peut voir que notre équation différentielle y ressemble en prenant $y = g$ et $h' = a$. On observe donc que
$$f'(x) = (y'(x) + a(x)y(x))\mathrm{e}^{h(x)} = 0$$
d'où
$$f(x) = \lambda$$
où $\lambda$ est une constante : on a donc $y(x) = \lambda\mathrm{e}^{-h(x)}$
\\\\
On obtient donc que $S = \mathrm{Vect}(\mathrm{e}^{-h(x)})$ et que donc $S$ est de dimension 1 : c'est une droite !

\end{document}
